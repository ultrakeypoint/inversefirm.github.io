"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[55],{1042:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"api/llmboost","title":"llmboost","description":"LLMBoost Objects","source":"@site/docs/api/llmboost.md","sourceDirName":"api","slug":"/api/llmboost","permalink":"/api/llmboost","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"API Documentation","permalink":"/category/api-documentation"},"next":{"title":"Parallelism","permalink":"/parallelism"}}');var i=t(4848),l=t(8453);const o={},r="llmboost",a={},d=[{value:"LLMBoost Objects",id:"llmboost-objects",level:2},{value:"__init__",id:"__init__",level:4},{value:"start",id:"start",level:4},{value:"stop",id:"stop",level:4},{value:"aget_output",id:"aget_output",level:4},{value:"get_output",id:"get_output",level:4},{value:"apply_format",id:"apply_format",level:4},{value:"set_query_type",id:"set_query_type",level:4}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)("a",{id:"llmboost"}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"llmboost",children:"llmboost"})}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost"}),"\n",(0,i.jsx)(n.h2,{id:"llmboost-objects",children:"LLMBoost Objects"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class LLMBoost()\n"})}),"\n",(0,i.jsx)(n.p,{children:"LLMBoost is a ready-to-deploy, full stack AI inferencing server offering unprecedented performance, cost efficiency and flexibility."}),"\n",(0,i.jsx)(n.p,{children:"This class is used to interact with the LLMBoost Runtime.\nIt provides methods to start, stop, issue inputs, get outputs, set the query type and format the inputs."}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost.__init__"}),"\n",(0,i.jsx)(n.h4,{id:"__init__",children:"__init__"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def __init__(model_name,\n             query_type="text",\n             tp=0,\n             dp=0,\n             max_num_seqs=256,\n             max_model_len=None,\n             load_format="auto",\n             quantization=None,\n             kv_cache_dtype="auto",\n             quantization_param_path=None,\n             quantization_weight_path=None,\n             enable_async_output=False,\n             streaming=False,\n             beam_width=0,\n             max_tokens=1024,\n             drain_per_worker=False,\n             load_balancing_mode="auto",\n             first_gpu_id=0,\n             vllm_args={})\n'})}),"\n",(0,i.jsx)(n.p,{children:"Initialize the LLMBoost Runtime."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model_name"})," ",(0,i.jsx)(n.em,{children:"str"})," - The name of the model to be used. This can be a model from the Hugging Face Model Hub or a custom model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"query_type"})," ",(0,i.jsx)(n.em,{children:"str, List[Int]"}),' - The type of query to be used. This can be either "text" or "tokens".']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"tp"})," ",(0,i.jsx)(n.em,{children:"int"})," - Tensor Parallelism."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dp"})," ",(0,i.jsx)(n.em,{children:"int"})," - Data Parallelism."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_num_seqs"})," ",(0,i.jsx)(n.em,{children:"int"})," - Maximum number of sequences to be processed in parallel."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_model_len"})," ",(0,i.jsx)(n.em,{children:"int"})," - Maximum number of model tokens."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"load_format"})," ",(0,i.jsx)(n.em,{children:"str"}),' - The format in which the model is loaded. The default is "auto" but you can specify either "pt" and "safetensor" if you desire.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"quantization"})," ",(0,i.jsx)(n.em,{children:"str"}),' - The quantization method to be used. The default is None but you can specify "fp8" to use floating point 8-bit quantization.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"kv_cache_dtype"})," ",(0,i.jsx)(n.em,{children:"str"}),' - The data type to be used for the key-value cache. The default is "auto" but you can specify "fp8" to use floating point 8-bit quantization.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"quantization_param_path"})," ",(0,i.jsx)(n.em,{children:"str"})," - The path to the quantization parameters."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"quantization_weight_path"})," ",(0,i.jsx)(n.em,{children:"str"})," - The path to the quantization weights."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"enable_async_output"})," ",(0,i.jsx)(n.em,{children:"bool"})," - This enable asyncio support for the output queue."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"streaming"})," ",(0,i.jsx)(n.em,{children:"bool"})," - This enables streaming mode."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"beam_width"})," ",(0,i.jsx)(n.em,{children:"int"})," - The beam width to be used for beam search."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"max_tokens"})," ",(0,i.jsx)(n.em,{children:"int"})," - The maximum number of tokens to be generated."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"drain_per_worker"})," ",(0,i.jsx)(n.em,{children:"bool"})," - This enables draining per worker, this can improve latency in rare cases but it not recommended."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"load_balancing_mode"})," ",(0,i.jsx)(n.em,{children:"str"}),' - The load balancing mode to be used. The default is "auto" but you can specify "passthrough" or "scatter".']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"first_gpu_id"})," ",(0,i.jsx)(n.em,{children:"int"})," - The first GPU ID to be used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vllm_args"})," ",(0,i.jsx)(n.em,{children:"dict"})," - The arguments to be passed to the VLLM."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"Create an LLMBoost Object with Llama-3.1-70B-Instruct model with default parameters."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'llm = LLMBoost(model_name="meta-llama/Llama-3.1-70B-Instruct")\n'})}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost.start"}),"\n",(0,i.jsx)(n.h4,{id:"start",children:"start"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def start()\n"})}),"\n",(0,i.jsx)(n.p,{children:"Start the LLMBoost Runtime."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"Start the LLMBoost Runtime."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"llm.start()\n"})}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost.stop"}),"\n",(0,i.jsx)(n.h4,{id:"stop",children:"stop"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def stop()\n"})}),"\n",(0,i.jsx)(n.p,{children:"Stop the LLMBoost Runtime."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"Stop the LLMBoost Runtime."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"llm.stop()\n"})}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost.aget_output"}),"\n",(0,i.jsx)(n.h4,{id:"aget_output",children:"aget_output"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"async def aget_output()\n"})}),"\n",(0,i.jsx)(n.p,{children:"Asyncio compatible get_output method."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Dict"})," - The output."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"Get the output from the LLMBoost Runtime."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"out = await llm.aget_output()\n"})}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost.get_output"}),"\n",(0,i.jsx)(n.h4,{id:"get_output",children:"get_output"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def get_output(num=int(1e9))\n"})}),"\n",(0,i.jsx)(n.p,{children:"Get the output from the LLMBoost Runtime.\nThe output dictionary is in the format:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'{\n    "id": int,\n    "val": str,\n    "finished": dict\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num"})," ",(0,i.jsx)(n.em,{children:"int"})," - The number of outputs to get."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"List[Dict]"})," - The list of outputs."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"Get the output from the LLMBoost Runtime."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'out = llm.get_output()\n\nfor o in out:\n    # Is the request finished?\n    print(o["finished"])\n\n    # Print the id\n    print(o["id"])\n\n    # Print the predicted text\n    print(o["val"])\n'})}),"\n",(0,i.jsx)(n.p,{children:"This is a simple example of how one would use the streaming outputs:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'preds = {}\n\nout_count = 0\nwhile out_count < len(inputs):\n    outputs = llmboost.get_output()\n    for o in outputs:\n        preds[o["id"]] = preds.get(o["id"], "") + o["val"]\n        out_count += o["finished"]\n\n<a id="llmboost.LLMBoost.issues_inputs"></a>\n\n#### issues\\_inputs\n\n```python\ndef issues_inputs(inputs)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Issues inputs to the LLMBoost Runtime."}),"\n",(0,i.jsx)(n.p,{children:"The input dictionary is in the format:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'{\n    "id": id,\n    "val": str\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"inputs"})," ",(0,i.jsx)(n.em,{children:"Dict, List[Dict]"})," - The inputs to be issued."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"Issue inputs to the LLMBoost Runtime."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'text = "What is the largest planet in the solar system?"\nllm.issues_inputs({"id": 1, "val": text})\n'})}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost.apply_format"}),"\n",(0,i.jsx)(n.h4,{id:"apply_format",children:"apply_format"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def apply_format(chat)\n"})}),"\n",(0,i.jsx)(n.p,{children:"Apply the model specific format to the chat."}),"\n",(0,i.jsx)(n.p,{children:"The input chat dictionary is in the format:"}),"\n",(0,i.jsx)(n.p,{children:"The output dictionary is in the format:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'{\n    "role": str,\n    "content": str\n}\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'{\n    "id": int,\n    "val": str\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"chat"})," ",(0,i.jsx)(n.em,{children:"str, Dict, List[Dict]"})," - The chat to be formatted."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Dict"})," - The formatted chat."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"Apply the model specific format to the chat."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"llm = LLMBoost(model_name=\"meta-llama/Llama-3.1-70B-Instruct\")\nformatted_chat = llm.apply_format(\n[\n    {'role': 'system', 'content': 'You are a chatbot.'},\n    {'role': 'user', 'content': 'What is the largest planet in the solar system?'}\n]\n\nprint(formatted_chat)\n>>> {'id': 123456789, 'val': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a chatbot.<|eot_id|><|start_header_id|>user<|end_header_id|>What is the largest planet in the solar system?<|eot_id|>><|start_header_id|>assistant<|end_header_id|>'}\n"})}),"\n",(0,i.jsx)("a",{id:"llmboost.LLMBoost.set_query_type"}),"\n",(0,i.jsx)(n.h4,{id:"set_query_type",children:"set_query_type"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def set_query_type(query_type="tokens")\n'})}),"\n",(0,i.jsx)(n.p,{children:"This method is used to set the query type."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"query_type"})," ",(0,i.jsx)(n.em,{children:"str"}),' - The query type to be used. This can be either "text" or "tokens".']}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsx)(n.p,{children:'Set the query type to "tokens".'}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'llm.set_query_type("tokens")\n'})})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(6540);const i={},l=s.createContext(i);function o(e){const n=s.useContext(l);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);