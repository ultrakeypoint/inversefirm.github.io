"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[88],{4501:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"tutorial","title":"Tutorial","description":"This tutorial demonstrates how to use the In-Process SDK of LLMBoost so that you can integrated it into your own application.","source":"@site/docs/tutorial.mdx","sourceDirName":".","slug":"/tutorial","permalink":"/tutorial","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Mango LLMBoost","permalink":"/"},"next":{"title":"Inference Server","permalink":"/inference-server"}}');var o=n(4848),r=n(8453);const i={sidebar_position:2},a="Tutorial",l={},c=[];function d(e){const t={a:"a",admonition:"admonition",code:"code",h1:"h1",header:"header",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"tutorial",children:"Tutorial"})}),"\n",(0,o.jsx)(t.p,{children:"This tutorial demonstrates how to use the In-Process SDK of LLMBoost so that you can integrated it into your own application."}),"\n",(0,o.jsx)(t.admonition,{title:"Only interested in the Inference Server?",type:"tip",children:(0,o.jsxs)(t.p,{children:["If you plan on using LLMBoost only as an inference server feel free to skip straight to ",(0,o.jsx)(t.a,{href:"/inference-server",children:"Inference Server"})]})}),"\n",(0,o.jsxs)(t.p,{children:["If you are using one of the Cloud-based LLMBoost images the contents of the code listings can be found in ",(0,o.jsx)(t.code,{children:"tutorial.py"}),"."]}),"\n",(0,o.jsx)(t.p,{children:"First we import the relevant packages:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:"# Copyright 2024, MangoBoost, Inc. All rights reserved.\n\nimport pickle\nimport time\n\nfrom llmboost import LLMBoost\n\nimport evaluate\nimport nltk\nfrom transformers import AutoTokenizer\nfrom tqdm import tqdm\n\nimport numpy as np\n\nfrom absl import app\n"})}),"\n",(0,o.jsxs)(t.p,{children:["The LLMBoost runtime is imported using ",(0,o.jsx)(t.code,{children:"from llmboost import LLMBoost"}),"."]}),"\n",(0,o.jsxs)(t.p,{children:["The tutorial is an ",(0,o.jsx)(t.code,{children:"absl"})," application leveraging the LLMBoost runtime which performs text completion.\nThe inputs and expected outputs are taken from the open ORCA dataset.\nAt the end of inference the ROUGE score is calculated for the 1000 samples.\nAs standard in ",(0,o.jsx)(t.code,{children:"absl"})," applications we start with ",(0,o.jsx)(t.code,{children:"main"})," and ",(0,o.jsx)(t.code,{children:"__main__"})," declarations:"]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'def main(_):\n    ...\n\nif __name__ == "__main__":\n    app.run(main)\n'})}),"\n",(0,o.jsx)(t.admonition,{title:"Simple Example",type:"tip",children:(0,o.jsxs)(t.p,{children:["All of the code from this point on is placed in the ",(0,o.jsx)(t.code,{children:"def main(_)"})," function."]})}),"\n",(0,o.jsxs)(t.p,{children:["This snippet below reads a pre-processed version of the open ORCA dataset (saved in a ",(0,o.jsx)(t.code,{children:"pkl"})," file) and extracts the system and user prompts in addition to the expected results.\nIf you are interested in how the dataset was processed please refer to ",(0,o.jsx)(t.a,{href:"https://github.com/mlcommons/inference/tree/master/language/llama2-70b#unprocessed",children:"this resource"}),"."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'# The dataset is a pickled pandas DataFrame with the following columns:\n# - system_prompt: A system-generated prompt.\n# - question: A user-generated question.\n# - output: The reference answer to the question.\ndataset_path = "./llama2_data.pkl"\n\nwith open(dataset_path, "rb") as f:\n    dataset = pickle.load(f)\n\nsource_texts = []\nfor _, row in dataset.iterrows():\n    system = row.get("system_prompt", None)\n    user = row.get("question", None)\n    source_texts.append({"system": system, "user": user})\n\ntarget_texts = dataset["output"].tolist()\nprint(f"Loaded dataset with {len(source_texts)} samples")\n'})}),"\n",(0,o.jsxs)(t.p,{children:["Now it is time to setup the LLMBoost runtime.\nLLMBoost is an optimized inference runtime that exposes multiple levels of parallelism.\nBy default LLMBoost to try to determine the best parallelism to maximize throughput, however Data Parallelism and Tensor Parallelism can be control directly, please refer to the API documentation on how to do this.\nFor additional information on Data Parallelism and Tensor Parallelism please refer to the section on ",(0,o.jsx)(t.a,{href:"/parallelism",children:"Parallelism"})]}),"\n",(0,o.jsxs)(t.p,{children:["In this snippet we name our target model, in this case ",(0,o.jsx)(t.code,{children:"microsoft/Phi-3-mini-4k-instruct"}),", and specify the ",(0,o.jsx)(t.code,{children:"max_tokens"})," we want to generate.\nFinally we call ",(0,o.jsx)(t.code,{children:"llmboost.start()"})," to prepare the runtime to start accepting inference requests."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'model_name = "microsoft/Phi-3-mini-4k-instruct"\n\n# Instantiate LLMBoost with the specified parameters.\nllmboost = LLMBoost(model_name=model_name, max_tokens=1024)\n\n# Start the LLMBoost runtime.\nllmboost.start()\n'})}),"\n",(0,o.jsx)(t.admonition,{title:"HuggingFace or Local",type:"tip",children:(0,o.jsxs)(t.p,{children:["LLMBoost supports loading models in multiple formats including download models directly from ",(0,o.jsx)(t.a,{href:"https://huggingface.co/",children:"HuggingFace"}),"."]})}),"\n",(0,o.jsxs)(t.p,{children:["Now that the LLMBoost runtime has been started we can format the inputs and issue them to the LLMBoost runtime.\nIn the case of the Open ORCA dataset there is a ",(0,o.jsx)(t.code,{children:"system"})," and ",(0,o.jsx)(t.code,{children:"user"})," message and the exact format of of these prompts often depends on the particular model.\nLLMBoost attempts apply the model specific format with ",(0,o.jsx)(t.code,{children:"llmboost.apply_format(...)"}),", alternatively a user can build their own format and submit the requests as a list of requests (",(0,o.jsx)(t.code,{children:"List[Dict[int, str]]"}),").\nSince LLMBoost is design for streaming LLM Inference there are separate thread-safe function calls for issuing requests and collecting results.\nIn a production environment we recommend using different threads for issuing and collecting.\nThe ",(0,o.jsx)(t.code,{children:"llmboost.get_output()"})," function is blocking request that will sleep until data is available.\nFor more information on the specifics, please refer to the ",(0,o.jsx)(t.a,{href:"/api/llmboost",children:"API documentation"}),"."]}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'num_prompts = 1000\n\n# The apply_format function formats the input text segments into the required format.\ninputs = []\nfor i, p in enumerate(source_texts[:num_prompts]):\n    inputs.append(\n        llmboost.apply_format(\n            [\n                {"role": "system", "content": p["system"]},\n                {"role": "user", "content": p["user"]},\n            ]\n        )\n    )\n\n    # Assign an id to each input for tracking the output.\n    inputs[-1]["id"] = i\n\n# Issue the inputs to LLMBoost.\n# The input format is a list of dictionaries with the following keys:\n# - id: A unique identifier for the input.\n# - val: The input text.\nllmboost.issues_inputs(inputs)\n'})}),"\n",(0,o.jsx)(t.p,{children:"This next snippet collects the results from LLMBoost as they become available.\nLLMBoost supports streaming of incomplete results, this tutorial doesn't cover this aspect so please refer to the API documentation if you are interested."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'# Query LLMBoost for the outputs.\n# The outputs are returned as a list of dictionaries with the following keys:\n# - id: The unique identifier of the input.\n# - val: The output text.\n# - finished: A flag indicating if the output is complete. (Only relevant for streaming mode)\npreds = {}\nout_count = 0\nwhile out_count < len(inputs):\n    outputs = llmboost.get_output()\n    for q in outputs:\n        preds[q["id"]] = q["val"]\n        out_count += 1\n'})}),"\n",(0,o.jsx)(t.p,{children:"Now that all of the results have been collected we can stop the runtime."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:"llmboost.stop()\n"})}),"\n",(0,o.jsx)(t.admonition,{title:"Performance",type:"tip",children:(0,o.jsxs)(t.p,{children:["The ",(0,o.jsx)(t.code,{children:"tutorial.py"})," provided with the LLMBoost release also has additional code to measure performance, this has been left out to only focus on the parts relevant to run LLMBoost."]})}),"\n",(0,o.jsx)(t.p,{children:"This final section compares the output of LLMBoost to the expected output and calculates the ROUGE score."}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-python",children:'# Sort the outputs based on the id.\npreds = [preds[key] for key in sorted(preds.keys(), reverse=False)]\n\n# Calculate the ROUGE scores.\nnltk.download("punkt", quiet=True)\nnltk.download("punkt_tab", quiet=True)\nmetric = evaluate.load("rouge")\n\n# Do some postprocessing to the texts.\npreds = [pred.strip() for pred in preds]\ntarget_texts = [target.strip() for target in target_texts[0 : len(preds)]]\n\n# rogueLSum expects newline after each sentence.\npreds = ["\\n".join(nltk.sent_tokenize(pred)) for pred in preds]\ntarget_texts = ["\\n".join(nltk.sent_tokenize(target)) for target in target_texts]\n\n# Calculate the ROUGE scores.\nresult = metric.compute(\n    predictions=preds,\n    references=target_texts,\n    use_stemmer=True,\n    use_aggregator=False,\n)\nresult = {k: round(np.mean(v) * 100, 4) for k, v in result.items()}\n\nprint(f"ROUGE1: {result[\'rouge1\']}")\nprint(f"ROUGE2: {result[\'rouge2\']}")\nprint(f"ROUGEL: {result[\'rougeL\']}")\nprint(f"ROUGELsum: {result[\'rougeLsum\']}")\n'})})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>a});var s=n(6540);const o={},r=s.createContext(o);function i(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);